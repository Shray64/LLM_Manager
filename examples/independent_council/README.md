# Independent Council Idea Generation Bot

A simpler idea generation system where each LLM in the council works independently, maintaining its own conversation history and being evaluated separately by a judge.

## How It Works

1. **User sends a question** - The question is sent to all LLMs in the council
2. **Each LLM generates ideas independently** - Each LLM maintains its own conversation history
3. **Individual evaluation** - Each response from each LLM is separately evaluated by the judge LLM
4. **Independent stopping** - LLMs stop when the judge scores their response below thresholds
5. **No sharing** - Responses are NOT shared between LLMs

## Key Differences from the Chairman-Based System

- **No Chairman**: There's no chairman LLM selecting responses
- **Independent Histories**: Each LLM maintains its own conversation history
- **Individual Evaluation**: Each response is evaluated separately
- **Parallel Execution**: All LLMs run in parallel, each with their own stopping condition

## Files

- `independent_bot.py` - Core bot logic with LLM state management
- `prompts.py` - System prompts for judge and council members
- `streamlit_app.py` - Streamlit web application for interactive use
- `__init__.py` - Package initialization

## Usage

### Streamlit Application

Run the Streamlit app:

```bash
streamlit run examples/independent_council/streamlit_app.py
```

The app allows you to:
- Enter a question
- Select which models to use in the council
- Set max iterations per LLM
- Adjust novelty and coherence thresholds
- See responses from each LLM in parallel columns
- View judge evaluations for each response

### Programmatic Usage

```python
from llm_manager import LLMManager
from independent_council.independent_bot import IndependentCouncilBot

# Initialize
llm_manager = LLMManager()
bot = IndependentCouncilBot(
    llm_manager,
    judge_model="bedrock/claude-sonnet-3.7",
    novelty_threshold=0.15,
    coherence_threshold=15
)

# Generate ideas
import asyncio
results = asyncio.run(bot.generate_ideas(
    question="How can we improve renewable energy adoption?",
    models=["azure/o3", "bedrock/claude-sonnet-3.7"],
    max_iterations=10
))

# Access results
for model, llm_state in results.items():
    print(f"{model}: {len(llm_state.responses)} ideas")
    for resp in llm_state.responses:
        print(f"  Idea: {resp['idea']}")
        print(f"  Novelty: {resp['novelty']}, Coherence: {resp['coherence']}")
```

## Configuration

Default models and thresholds can be modified in `independent_bot.py`:

- `DEFAULT_MODELS` - List of models to use in the council
- `DEFAULT_JUDGE_MODEL` - Model used for evaluation
- `DEFAULT_NOVELTY_THRESHOLD` - Minimum novelty score (0.0-1.0)
- `DEFAULT_COHERENCE_THRESHOLD` - Minimum coherence score (0-100)

## Output Structure

Each LLM's state contains:
- `conversation_history`: List of all ideas generated by this LLM
- `responses`: List of response dictionaries with:
  - `idea`: The generated idea text
  - `novelty`: Novelty score from judge
  - `coherence`: Coherence score from judge
  - `reasoning`: Judge's reasoning
  - `iteration`: Iteration number when generated
- `is_active`: Whether the LLM is still generating ideas



